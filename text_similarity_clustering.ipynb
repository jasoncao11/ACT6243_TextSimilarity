{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Similarity and Clustering\n",
    "The goal of this lab is to illustrate how to calculate similarities between documents with Gensim in Python, we will also learn how to perform k-means clustering with sklearn.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1. Text Similarity with Gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is our corpus. It consists of 7 documents, where each document is a string consisting of a single sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_corpus = [\n",
    "    \"美的格力大战胜负初显，美的市场份额大涨但打败格力就真是胜利？\",\n",
    "    \"家电巨头上半年：美的格力互撕，海尔默默掉队？\",\n",
    "    \"环球时报：抓孟晚舟开了极其恶劣的先例，中国不能怯懦。\",\n",
    "    \"业绩不佳拖累股价，巴菲特公司减仓苹果股票290万股。\",\n",
    "    \"鹰眼预警：美的集团应收账款增速高于营业收入增速。\",\n",
    "    \"消息称滴滴2018年亏了109亿，创始人称6年来没盈利过。\",\n",
    "    \"小米布局出行领域\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Write code to implement the following:\n",
    "- Tokenize the documents into words, remove stop words, save the result as processed_corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba\n",
    "STOP_WORDS = {'，','。','：','？'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(corpus):\n",
    "    processed_corpus = []\n",
    "    ### START CODE HERE ###\n",
    "\n",
    "    ### END CODE HERE ###\n",
    "    return processed_corpus        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check your function\n",
    "processed_corpus = tokenize(text_corpus)\n",
    "print(processed_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Expected output\n",
    "```\n",
    "[['美的', '格力', '大战', '胜负', '初显', '美的', '市场份额', '大涨', '但', '打败', '格力', '就', '真是', '胜利'], ['家电', '巨头', '上半年', '美的', '格力', '互', '撕', '海尔', '默默', '掉队'], ['环球时报', '抓孟晚', '舟开', '了', '极其', '恶劣', '的', '先例', '中国', '不能', '怯懦'], ['业绩', '不佳', '拖累', '股价', '巴菲特', '公司', '减仓', '苹果', '股票', '290', '万股'], ['鹰眼', '预警', '美的', '集团', '应收', '账款', '增速', '高于', '营业', '收入', '增速'], ['消息', '称', '滴滴', '2018', '年', '亏了', '109', '亿', '创始人', '称', '6', '年', '来', '没', '盈利', '过'], ['小米', '布局', '出行', '领域']]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Create a Gensim Dictionary that maps tokens to indices based on observed order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora\n",
    "### START CODE HERE ###\n",
    "dictionary = \n",
    "### END CODE HERE ###\n",
    "print(dictionary.token2id)\n",
    "print (f'There are {len(dictionary.token2id)} words in the dictionary.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Expected output\n",
    "```\n",
    "{'但': 0, '初显': 1, '大战': 2, '大涨': 3, '就': 4, '市场份额': 5, '打败': 6, '格力': 7, '真是': 8, '美的': 9, '胜利': 10, '胜负': 11, '上半年': 12, '互': 13, '家电': 14, '巨头': 15, '掉队': 16, '撕': 17, '海尔': 18, '默默': 19, '不能': 20, '中国': 21, '了': 22, '先例': 23, '怯懦': 24, '恶劣': 25, '抓孟晚': 26, '极其': 27, '环球时报': 28, '的': 29, '舟开': 30, '290': 31, '万股': 32, '不佳': 33, '业绩': 34, '公司': 35, '减仓': 36, '巴菲特': 37, '拖累': 38, '股价': 39, '股票': 40, '苹果': 41, '增速': 42, '应收': 43, '收入': 44, '营业': 45, '账款': 46, '集团': 47, '预警': 48, '高于': 49, '鹰眼': 50, '109': 51, '2018': 52, '6': 53, '亏了': 54, '亿': 55, '创始人': 56, '年': 57, '来': 58, '没': 59, '消息': 60, '滴滴': 61, '盈利': 62, '称': 63, '过': 64, '出行': 65, '小米': 66, '布局': 67, '领域': 68}\n",
    "There are 69 words in the dictionary.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Write code to implement the following:\n",
    "- Create the bag-of-words representation for our entire original corpus using the doc2bow method of the dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### START CODE HERE ###\n",
    "bow_corpus = \n",
    "### END CODE HERE ###\n",
    "print(bow_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Expected output\n",
    "```\n",
    "[[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 2), (8, 1), (9, 2), (10, 1), (11, 1)], [(7, 1), (9, 1), (12, 1), (13, 1), (14, 1), (15, 1), (16, 1), (17, 1), (18, 1), (19, 1)], [(20, 1), (21, 1), (22, 1), (23, 1), (24, 1), (25, 1), (26, 1), (27, 1), (28, 1), (29, 1), (30, 1)], [(31, 1), (32, 1), (33, 1), (34, 1), (35, 1), (36, 1), (37, 1), (38, 1), (39, 1), (40, 1), (41, 1)], [(9, 1), (42, 2), (43, 1), (44, 1), (45, 1), (46, 1), (47, 1), (48, 1), (49, 1), (50, 1)], [(51, 1), (52, 1), (53, 1), (54, 1), (55, 1), (56, 1), (57, 2), (58, 1), (59, 1), (60, 1), (61, 1), (62, 1), (63, 2), (64, 1)], [(65, 1), (66, 1), (67, 1), (68, 1)]]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "Now that we have vectorized our corpus we can begin to transform it using tf-idf model. The tf-idf model transforms vectors from the bag-of-words representation to a vector space where the frequency counts are weighted according to the relative rarity of each word in the corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Write code to implement the following:\n",
    "- Initialize the tf-idf model, training it on our corpus.\n",
    "- Transform query_document_2 according to the method of transforming query_document_1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import models\n",
    "\n",
    "# train the model\n",
    "### START CODE HERE ###\n",
    "tfidf = \n",
    "### END CODE HERE ###\n",
    "\n",
    "# transform the queries\n",
    "query_document_1 = '家电三巨头半年报：美的优势凸显,格力,海尔以改革赌明天'\n",
    "query_document_2 = '雷军入股共享单车'\n",
    "tokenized_query_document_1 = [word for word in list(jieba.cut(query_document_1)) if word not in STOP_WORDS]\n",
    "tfidf_query_document_1 = tfidf[dictionary.doc2bow(tokenized_query_document_1)]\n",
    "### START CODE HERE ###\n",
    "tokenized_query_document_2 = \n",
    "tfidf_query_document_2 =\n",
    "### END CODE HERE ###\n",
    "print (f'TFIDF representation of query_document_1 is {tfidf_query_document_1}')\n",
    "print (f'TFIDF representation of query_document_2 is {tfidf_query_document_2}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Expected output\n",
    "```\n",
    "TFIDF representation of query_document_1 is [(7, 0.3391172420540524), (9, 0.22935968003421267), (14, 0.526749033638176), (15, 0.526749033638176), (18, 0.526749033638176)]\n",
    "TFIDF representation of query_document_2 is []\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Write code to implement the following:\n",
    "- Transform the whole corpus via TfIdf and index it, in preparation for similarity queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import similarities\n",
    "### START CODE HERE ###\n",
    "index = \n",
    "### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query the similarity of query_document_1 against every document in the corpus\n",
    "sims = index[tfidf_query_document_1]\n",
    "for document_number, score in sorted(enumerate(sims), key=lambda x: x[1], reverse=True):\n",
    "    print(document_number, score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Expected output\n",
    "```\n",
    "1 0.6472088\n",
    "0 0.18060154\n",
    "4 0.028604593\n",
    "2 0.0\n",
    "3 0.0\n",
    "5 0.0\n",
    "6 0.0\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Query the similarity of query_document_2 against every document in the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### START CODE HERE ###\n",
    "\n",
    "### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Expected output\n",
    "```\n",
    "0 0.0\n",
    "1 0.0\n",
    "2 0.0\n",
    "3 0.0\n",
    "4 0.0\n",
    "5 0.0\n",
    "6 0.0\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2. Word Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In above case, we can see the similarity between query_document_2 and each document in our corpus is zero, but if we look into the corpus, we can find out it is high likely that query_document_2(雷军入股共享单车) and the last document(小米布局出行领域) of the corpus are talking the same thing even though they do not share any words. To overcome this limitation of representing document either by BOW or TF-IDF, we can take advantage of word emdedding,  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Write code to implement the following:\n",
    "- Use the word2vec model we have trained before to convert each doc into a vevtor. This can be achieved by transforming each word occurs in the doc to its corresponding vector and taking average of all the word vectors, this final vector will be used to represent the doc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "doc_1 = '雷军入股共享单车'\n",
    "doc_2 = '小米布局出行领域'\n",
    "\n",
    "### START CODE HERE ###\n",
    "\n",
    "### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3. K-means Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the matrix doc_vecs shows, we have 30 documents and each document is represented as a 2-dimensional vector, doc_ids is a list that contains the corresponding id of each document. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Write code to implement the following:\n",
    "- Use k-means to group the documents into 3 clusters.\n",
    "- Output the document ids that contained in each cluster\n",
    "- Check out <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html\">KMeans documentation</a> for any help."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_vecs = np.array([[-0.81082279,  4.76132034],\n",
    "       [-1.54663157,  3.21771457],\n",
    "       [-1.99762633,  2.66619505],\n",
    "       [-1.88446761,  2.09143973],\n",
    "       [ 2.16373072,  1.16237742],\n",
    "       [ 2.02212543,  4.16017654],\n",
    "       [ 1.4338175 , -0.48889387],\n",
    "       [-2.15373061,  3.18871401],\n",
    "       [ 1.69851099,  0.59101165],\n",
    "       [-1.97092948,  2.66396345],\n",
    "       [-2.65663251,  2.76896407],\n",
    "       [-1.40270571,  2.63663561],\n",
    "       [-1.48034196,  3.12961259],\n",
    "       [ 2.916471  ,  1.73932955],\n",
    "       [ 1.19541747,  3.70592031],\n",
    "       [ 1.64133197,  4.19783728],\n",
    "       [ 1.78413875,  0.68605173],\n",
    "       [ 1.07710058,  5.32177878],\n",
    "       [-2.09610641,  1.70948444],\n",
    "       [ 1.58137542,  3.78427181],\n",
    "       [ 3.12821297,  1.9262148 ],\n",
    "       [ 2.08729848,  0.76663496],\n",
    "       [ 1.28697434,  4.53735936],\n",
    "       [ 0.90401688,  4.59120628],\n",
    "       [ 2.56509832,  3.28573136],\n",
    "       [ 1.81172902,  1.00710794],\n",
    "       [ 0.86087839,  2.26320644],\n",
    "       [ 1.32128045, -0.0963489 ],\n",
    "       [-2.40386077,  3.46212551],\n",
    "       [ 1.50899649,  4.38895984]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_ids = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from collections import defaultdict\n",
    "### START CODE HERE ###\n",
    "\n",
    "### END CODE HERE ###"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
